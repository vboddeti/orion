import time
import math
from typing import Union, Dict, Any

import yaml
import torch
from tqdm import tqdm
from torch.utils.data import DataLoader, RandomSampler
import logging

from orion.nn.module import Module
from orion.nn.linear import LinearTransform
from orion.backend.lattigo import bindings as lgo
from orion.backend.python import (
    parameters, 
    key_generator,
    encoder, 
    encryptor,
    evaluator, 
    poly_evaluator, 
    lt_evaluator,
    bootstrapper
)

from .tracer import OrionTracer, StatsTracker 
from .fuser import Fuser
from .network_dag import NetworkDAG
from .auto_bootstrap import BootstrapSolver, BootstrapPlacer
from .logger import logger as ORION_LOGGER


class Scheme:
    """
    This Scheme class drives most of the functionality in Orion. It 
    configures and manages how our framework interfaces with FHE backends, 
    and exposes this functionality to the user through attributes such as 
    the encoder, evaluators (linear transform, polynomial, etc.) and 
    bootstrappers. 

    It also serves two important purposes required before running FHE 
    inference: fitting the network and then compiling it. The fit() method 
    runs cleartext forward passes through the network to determine per-layer 
    input ranges, which are then used to fit polynomial approximations to 
    common activation functions (e.g., SiLU, ReLU). 

    The compile() function is responsible for all packing of data and 
    determines a level management policy by running our automatic bootstrap 
    placement algorithm. Once done, each Orion module is automatically 
    assigned a level that can then be used in its compilation. This primarily 
    includes generating the plaintexts needed for each linear transform. 
    """
    
    def __init__(self):
        self.backend = None
        self.trace = None

    def init_scheme(self, config: Union[str, Dict[str, Any]]):
        """Initializes the scheme."""
        if isinstance(config, str):
            try:
                with open(config, "r") as f:
                    config = yaml.safe_load(f)
            except FileNotFoundError:
                raise ValueError(f"Configuration file '{config}' not found.")
        elif not isinstance(config, dict):
            raise TypeError("Config must be a file path (str) or a dictionary.")
        
        self.params = parameters.NewParameters(config)
        self.backend = self.setup_backend(self.params)
        
        self.keygen = key_generator.NewKeyGenerator(self)
        self.encoder = encoder.NewEncoder(self)
        self.encryptor = encryptor.NewEncryptor(self)
        self.evaluator = evaluator.NewEvaluator(self)
        self.poly_evaluator = poly_evaluator.NewEvaluator(self)
        self.lt_evaluator = lt_evaluator.NewEvaluator(self)
        self.bootstrapper = bootstrapper.NewEvaluator(self)

        return self
    
    def delete_scheme(self):
        if self.backend:
            self.backend.DeleteScheme()
    
    def __del__(self):
        self.delete_scheme()
    
    def __str__(self):
        return str(self.params)
        
    def setup_backend(self, params):
        backend = params.get_backend()
        if backend == "lattigo":
            py_lattigo = lgo.LattigoLibrary()
            py_lattigo.setup_bindings(params)
            return py_lattigo
        elif backend in ("heaan", "openfhe"):
            raise ValueError(f"Backend {backend} not yet supported.")
        else:
            raise ValueError(
                f"Invalid {backend}. Set the backend to Lattigo until "
                f"further notice."
            )

    def encode(self, tensor, level=None, scale=None):
        self._check_initialization()
        return self.encoder.encode(tensor, level, scale)

    def decode(self, ptxt):
        self._check_initialization() 
        return self.encoder.decode(ptxt)

    def encrypt(self, ptxt):
        self._check_initialization() 
        return self.encryptor.encrypt(ptxt)

    def decrypt(self, ctxt):
        self._check_initialization()
        return self.encryptor.decrypt(ctxt)

    def fit(self, net, input_data, batch_size=128):
        self._check_initialization()

        net.set_scheme(self)
        net.set_margin(self.params.get_margin())

        # First we'll generate an FX symbolic trace of the network. This lets
        # us mimic a forward pass through the network while tracking useful
        # statistics below (shapes, ranges, etc) using the StatsTracker.
        tracer = OrionTracer()
        trace = tracer.trace_model(net)
        self.trace = trace

        # An additional batch size parameter can be passed here to speed up
        # the process of fitting data. Any tracked batch dimensions are reset 
        # to what the user specified in the YAML file immediately afterwards.
        temp_batch_size = batch_size
        user_batch_size = self.params.get_batch_size()

        stats_tracker = StatsTracker(trace, temp_batch_size, user_batch_size)
        
        # Get the location of the model (cpu, gpu, etc.) so that the data 
        # propagated below is sent to the correct device.
        param = next(iter(net.parameters()), None)
        device = param.device if param is not None else torch.device("cpu")
        
        print("\n{1} Finding per-layer input/output ranges and shapes...", flush=True)

        #-------------------------------------#
        #     Generate input/output ranges    #
        #-------------------------------------#
        
        # Now pass the input data (tensor, list of tensors, dataloader, 
        # list of dataloaders) to through the model while tracking stats.
        stats_tracker.propagate_all(input_data, device, show_progress=True)

        #-------------------------------------#
        #      Fit polynomial activations     #
        #-------------------------------------#

        # Now we can use the ranges we just obtained above to fit all
        # Chebyshev polynomial activation functions.
        print("\n{2} Fitting polynomials... ", flush=True)
        for module in net.modules():
            if hasattr(module, "fit") and callable(module.fit):
                module.fit()
        
    def compile(self, net):
        self._check_initialization()

        if self.trace is None:
            raise ValueError(
                "Network has not been fit yet! Before running orion.compile(net) "
                "you must run orion.fit(net, input_data)."
            )
                
        #------------------------------------------------#
        #   Build DAG representation of neural network   #
        #------------------------------------------------#

        network_dag = NetworkDAG(self.trace)
        network_dag.build_dag()

        # Before fusing, we'll instantiate our own Orion parameters (e.g. 
        # weights and biases) that can be fused/modified without affecting 
        # the original network's parameters. 
        for module in net.modules():
            if (hasattr(module, "init_orion_params") and 
                    callable(module.init_orion_params)):
                module.init_orion_params()

        #-------------------------------------#
        #       Resolve pooling kernels       #
        #-------------------------------------# 
        
        # AvgPools are implemented as grouped convolutions in Orion, which
        # are not passed arguments for the number of channels for consistency
        # with PyTorch. We must resolve this after the passes above use 
        # torch.nn.functional.
        for module in net.modules():
            if hasattr(module, "update_params") and callable(module.update_params):
                module.update_params()

        #------------------------------------------#
        #   Fuse Orion modules (Conv -> BN, etc)   #
        #------------------------------------------#

        enable_fusing = self.params.get_fuse_modules()
        if enable_fusing:
            fuser = Fuser(network_dag)
            fuser.fuse_modules()
            network_dag.remove_fused_batchnorms()

        #---------------------------------------------#
        #   Pack diagonals of all linear transforms   #
        #---------------------------------------------#

        # Then, we must ensure that there is no junk data left in the slots
        # of the final linear layer (leaking information about partials).
        # This would occur when using the hybrid embedding method. We could
        # use an additional level to zero things out, but instead, we'll
        # just force the last linear layer to use the "square" embedding 
        # method which solves this while consuming just one level (albeit 
        # usually for more ciphertext rotations).
        topo_sort = list(network_dag.topological_sort())

        last_linear = None
        for node in reversed(topo_sort):
            module = network_dag.nodes[node]["module"]
            if isinstance(module, LinearTransform):
                last_linear = node
                break

        # Now we can generate the diagonals 
        if ORION_LOGGER.getEffectiveLevel() != logging.INFO:
            print("\n{3} Generating matrix diagonals...", flush=True)
        for node in topo_sort:
            module = network_dag.nodes[node]["module"]
            if isinstance(module, LinearTransform):
                if ORION_LOGGER.getEffectiveLevel() != logging.INFO:
                    print(f"\nPacking {node}:")
                module.generate_diagonals(last=(node == last_linear))

        #------------------------------#
        #   Find and place bootstraps  # 
        #------------------------------#

        network_dag.find_residuals()
        network_dag.plot(save_path="network.png", figsize=(8,30)) # optional plot

        print("\n{4} Running bootstrap placement... ", end="", flush=True)
        start = time.time()
        l_eff = len(self.params.get_logq()) - 1
        btp_solver = BootstrapSolver(net, network_dag, l_eff=l_eff)
        input_level, num_bootstraps, bootstrapper_slots = btp_solver.solve()
        print(f"done! [{time.time()-start:.3f} secs.]", flush=True)
        print(f"├── Network requires {num_bootstraps} bootstrap "
            f"{'operation' if num_bootstraps == 1 else 'operations'}.")

        btp_solver.plot_shortest_path(
           save_path="network-with-levels.png", figsize=(8,30) # optional plot
        )

        btp_placer = BootstrapPlacer(net, network_dag)
        btp_placer.place_bootstraps()

        if bootstrapper_slots:
            start = time.time()
            slots_str = ", ".join([str(int(math.log2(slot))) for slot in bootstrapper_slots])
            print(f"├── Generating bootstrappers for logslots = {slots_str} ... ", 
                  end="", flush=True)
            
            # Generate the required (potentially sparse) bootstrappers.
            for slot_count in bootstrapper_slots:
                self.bootstrapper.generate_bootstrapper(slot_count)
            print(f"done! [{time.time()-start:.3f} secs.]")

        #------------------------------------------#
        #   Compile Orion modules in the network   #
        #------------------------------------------#

        print("\n{5} Compiling network layers...", flush=True)
        for node in topo_sort:
            node_attrs = network_dag.nodes[node]
            module = node_attrs["module"]
            if isinstance(module, Module):
                print(f"├── {node} @ level={module.level}", flush=True)
                if hasattr(module, 'compile') and callable(module.compile):
                    module.compile()

        # Compile container modules that have dynamically created submodules
        # These modules are not in the DAG but may contain HerPN instances
        # We need to pass the trace so they can look up levels from traced modules
        print("\nCompiling container modules...", flush=True)
        for module in net.modules():
            # Only compile modules that have a compile() method but weren't in the DAG
            if hasattr(module, 'compile') and callable(module.compile):
                # Check if this is a container module (not a leaf operation)
                module_name = module.__class__.__name__
                if module_name in ['HerPNConv', 'HerPNPool']:
                    print(f"├── Compiling {module_name}", flush=True)
                    # Pass the trace so modules can look up levels
                    if hasattr(module.compile, '__code__') and module.compile.__code__.co_argcount > 1:
                        module.compile(self.trace)
                    else:
                        module.compile()

        # Sync FHE-compiled attributes from traced modules to original model modules
        # This is necessary because FX tracing may create separate module instances
        print("\n{6} Syncing FHE attributes to original model...", flush=True)
        trace_modules = dict(self.trace.named_modules())
        net_modules = dict(net.named_modules())

        for name, trace_mod in trace_modules.items():
            if name in net_modules:
                net_mod = net_modules[name]
                # Check if they're the same object
                is_same = (id(trace_mod) == id(net_mod))

                # Sync level attribute (critical for correct FHE execution)
                # Levels are updated after bootstrap placement, so traced module has correct levels
                if hasattr(trace_mod, 'level'):
                    old_level = getattr(net_mod, 'level', None)
                    if old_level != trace_mod.level:
                        net_mod.level = trace_mod.level
                        same_str = "(same object)" if is_same else f"(trace={id(trace_mod)}, net={id(net_mod)})"
                        print(f"├── Synced level to {name}: {old_level} → {trace_mod.level} {same_str}", flush=True)

                # Sync depth attribute (also critical for correct level calculations)
                if hasattr(trace_mod, 'depth'):
                    old_depth = getattr(net_mod, 'depth', None)
                    if old_depth != trace_mod.depth:
                        net_mod.depth = trace_mod.depth
                        same_str = "(same object)" if is_same else f"(trace={id(trace_mod)}, net={id(net_mod)})"
                        print(f"├── Synced depth to {name}: {old_depth} → {trace_mod.depth} {same_str}", flush=True)

                # Copy FHE-encoded weights if they exist
                for attr_name in dir(trace_mod):
                    if attr_name.endswith('_fhe') and hasattr(trace_mod, attr_name):
                        fhe_attr = getattr(trace_mod, attr_name)
                        setattr(net_mod, attr_name, fhe_attr)
                        same_str = "(same object)" if is_same else f"(trace={id(trace_mod)}, net={id(net_mod)})"
                        print(f"├── Synced {attr_name} to {name} {same_str}", flush=True)

                # Sync all FHE plaintext-encoded attributes (_ptxt)
                # This includes BatchNorm (on_running_mean_ptxt, on_inv_running_std_ptxt, on_weight_ptxt, on_bias_ptxt)
                # and LinearTransform (on_bias_ptxt) attributes
                for attr_name in dir(trace_mod):
                    if attr_name.endswith('_ptxt') and hasattr(trace_mod, attr_name):
                        ptxt_attr = getattr(trace_mod, attr_name)
                        # Print level info if available (for debugging)
                        if hasattr(ptxt_attr, 'level') and callable(ptxt_attr.level):
                            trace_level = ptxt_attr.level()
                            net_level = getattr(net_mod, attr_name).level() if hasattr(net_mod, attr_name) and hasattr(getattr(net_mod, attr_name), 'level') else "N/A"
                            print(f"├── Syncing {attr_name}: trace_level={trace_level}, net_level_before={net_level}", flush=True)
                        setattr(net_mod, attr_name, ptxt_attr)
                        same_str = "(same object)" if is_same else f"(trace={id(trace_mod)}, net={id(net_mod)})"
                        print(f"├── Synced {attr_name} to {name} {same_str}", flush=True)

                # Sync transform_ids for LinearTransform modules
                if hasattr(trace_mod, 'transform_ids'):
                    net_mod.transform_ids = trace_mod.transform_ids
                    same_str = "(same object)" if is_same else f"(trace={id(trace_mod)}, net={id(net_mod)})"
                    print(f"├── Synced transform_ids to {name} {same_str}", flush=True)

        return input_level # level at which to encrypt the input.

    def _check_initialization(self):
        if self.backend is None:
            raise ValueError(
                "Scheme not initialized. Call `orion.init_scheme()` first.") 
        
scheme = Scheme()
